{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow version 1.3\n",
    "\n",
    "# personal note, CTC Tensorflow documentation is not at all clear and intuitive , so if confused refer to any example\n",
    "\n",
    "# trying to implement word level CTC\n",
    "\n",
    "# note for decoder, the input has to be in this format 3-D float Tensor sized [max_time x batch_size x num_classes]\n",
    "# it doesn't support time major option\n",
    "\n",
    "# from v4 onwards trying to integrate dataset api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from python_speech_features import mfcc\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audio_to_mfcc(fileurl):\n",
    "    rate, sig = wav.read(fileurl)\n",
    "    mfcc_feat = mfcc(sig,rate)\n",
    "#d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "#fbank_feat = logfbank(sig,rate)\n",
    "\n",
    "    return mfcc_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = audio_to_mfcc('/home/saurabh/Documents/tf_orange/tf_orange/data/test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_csv(line):\n",
    "       parsed_line = tf.decode_csv(line, [[\"\"],[\"\"]])\n",
    "       \n",
    "       print (type(parsed_line[1]))\n",
    "       \n",
    "       \n",
    "\n",
    "       return parsed_line[0] , parsed_line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 299, 13)\n"
     ]
    }
   ],
   "source": [
    "train_inputs = np.asarray(inputs[np.newaxis, :])\n",
    "train_inputs = (train_inputs - np.mean(train_inputs))/np.std(train_inputs)\n",
    "train_seq_len = [train_inputs.shape[1]]\n",
    "print(train_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we also need a fixed vocabulary \n",
    "import re\n",
    "\n",
    "word_dictionary = {'hello':0 , 'world':1 , ' ':2}\n",
    "\n",
    "\n",
    "def word_to_index(sentence):\n",
    "   \n",
    "    words = sentence.split(' ')\n",
    "    index_list=[]\n",
    "    for word in words:\n",
    "        if word in word_dictionary:\n",
    "           # print(word)\n",
    "            index_list.insert(len(index_list) , word_dictionary[word])\n",
    "            index_list.insert(len(index_list) , word_dictionary[' '])\n",
    "    index_list.pop()        \n",
    "    return index_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _read_py_function(audio, label):\n",
    "    audio =audio_to_mfcc(audio)\n",
    "   \n",
    "    return audio ,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.contrib.data.TextLineDataset(\"/home/saurabh/Documents/tf_orange/tf_orange/data/data.csv\")\n",
    "dataset=dataset.map(decode_csv)\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda audio, label: tuple(tf.py_func(\n",
    "        _read_py_function, [audio, label], [tf.double, label.dtype])))\n",
    "\n",
    "\n",
    "#dataset = dataset.padded_batch(2, padded_shapes=([299,13,None]))\n",
    "\n",
    "dataset=dataset.batch(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "training_init_op = iterator.make_initializer(dataset)\n",
    "\n",
    "item = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is required for CTC Loss\n",
    "# for it's input , first convert transcrition / ground truth to number representation \n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_transcript = word_to_index('hello world hello world')\n",
    "batch_list = []\n",
    "batch_list.insert(0,output_transcript)\n",
    "train_targets = sparse_tuple_from(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 100\n",
    "batch_size = 2\n",
    "display_step = 200\n",
    "num_features = 13\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 299 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 4 # hello , world , blank and space \n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([batch_size, 2 * num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell_fw = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    \n",
    "    lstm_cell_bw = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    \n",
    "    # Get lstm cell output\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_cell_fw, lstm_cell_bw, x,\n",
    "    dtype=tf.float32)\n",
    "    \n",
    "    #convert output shape (timesteps * batch * classes ) to (batch*timesteps*classes)\n",
    "    outputs=tf.transpose( outputs , [1, 0, 2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #temp=tf.convert_to_tensor(outputs)[:,-1,:]\n",
    "    \n",
    "   # temp=temp[:,-1,:]\n",
    "    #return outputs\n",
    "    res =  tf.matmul(outputs, weights['out']) + biases['out']\n",
    "    \n",
    "    return res\n",
    "   # return tf.nn.softmax(tf.matmul(outputs, weights['out']) + biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "targets = tf.sparse_placeholder(tf.int32)\n",
    "seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "logits = RNN(inputs, weights, biases)\n",
    "\n",
    "loss =  tf.nn.ctc_loss ( targets, logits , seq_len , time_major = False)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                           0.9).minimize(cost)\n",
    "\n",
    "\n",
    "# Option 2: tf.contrib.ctc.ctc_beam_search_decoder\n",
    "    # (it's slower but you'll get better results)\n",
    "decoder_input = tf.transpose(logits, [1, 0, 2])\n",
    "\n",
    "decoded, log_prob = tf.nn.ctc_greedy_decoder(decoder_input, seq_len)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need this for decoding word give its index key value \n",
    "\n",
    "def keys_of_value(dct, value):\n",
    "    for k in dct:\n",
    "        if isinstance(dct[k], list):\n",
    "            if value in dct[k]:\n",
    "                return k\n",
    "        else:\n",
    "            if value == dct[k]:\n",
    "                return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "293.452\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "208.393\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "77.3848\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "36.2902\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "21.1679\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "11.713\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "4.86216\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "4.18828\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "3.9112\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "1.4327\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.86509\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.797672\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.512631\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.350406\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.266259\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.23593\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.195685\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.165172\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.144621\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.122007\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0998137\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0824709\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0733542\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0734062\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.07462\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0726082\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0670154\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0603006\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0544459\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0498086\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.045716\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0421265\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0393438\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0372751\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.035659\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0342951\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0330682\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0319167\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0308068\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0297258\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0286696\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0276435\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.026652\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0257009\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0247947\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0239391\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0231351\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0223838\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0216852\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0210385\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.020443\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0198933\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0193894\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0189262\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0185009\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0181085\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0177455\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.017406\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0170898\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0167912\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.016508\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0162385\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0159814\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0157347\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0154967\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0152686\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0150483\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.014836\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0146313\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0144332\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0142432\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0140597\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.013883\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0137131\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0135494\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0133919\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0132399\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0130936\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0129523\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0128161\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0126847\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0125569\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0124336\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0123145\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0121984\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0120859\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0119764\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0118698\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0117654\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0116645\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0115651\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.011469\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0113746\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0112826\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0111923\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.011104\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0110181\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0109338\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.010851\n",
      "End of training dataset.\n",
      "(2, 299, 13)\n",
      "[0, 2, 1, 2, 0, 2, 1]\n",
      "[1, 2, 1, 2, 1]\n",
      "0.0107699\n",
      "End of training dataset.\n",
      "0.0215397458524\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "\n",
    "\n",
    "    #print(train_seq_len)\n",
    "    \n",
    "    for curr_epoch in range(training_steps):\n",
    "        train_cost = train_ler = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        while True: \n",
    "            try:\n",
    "            \n",
    "           \n",
    "                elem = sess.run(item)\n",
    "                print(elem[0].shape)\n",
    "                for label in elem[1]:\n",
    "                    print (word_to_index(label.decode(\"utf-8\")) )\n",
    "                \n",
    "                batch_inputs=elem[0]\n",
    "            \n",
    "            \n",
    "            \n",
    "                batch_list = []\n",
    "            \n",
    "                for index,j in enumerate(elem[1]):\n",
    "                    batch_list.insert(index, word_to_index(j.decode(\"utf-8\")))\n",
    "                \n",
    "                batch_targets = sparse_tuple_from(batch_list)\n",
    "            \n",
    "            \n",
    "                feed = {inputs: batch_inputs,\n",
    "                        targets: batch_targets,\n",
    "                        seq_len: [299,299]}\n",
    "\t    \n",
    "\n",
    "                batch_cost, _ = sess.run([cost, optimizer], feed)\n",
    "                print(batch_cost)\n",
    "                \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"End of training dataset.\")\n",
    "                break\n",
    "    \n",
    "        \n",
    "    train_cost += batch_cost*batch_size\n",
    "    print(train_cost)\n",
    "    \n",
    "    # Decoding\n",
    "#    d = sess.run(decoded[0], feed_dict=feed)\n",
    "#    str_decoded = ''.join([keys_of_value(word_dictionary, x) for x in np.asarray(d[1]) ])\n",
    "\n",
    "    \n",
    "#    print(str_decoded)\n",
    "#    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_orange",
   "language": "python",
   "name": "tf_orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
