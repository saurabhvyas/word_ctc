{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  word level CTC  tensorflow version 1.3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# to do :   \n",
    "#  , testing on realdataset , get rid of fixed batch size , instead use variable batch size  \n",
    "\n",
    "\n",
    "# perform following tests :\n",
    "\n",
    "# 1. variable batch sizes test\n",
    "# 2. GPU test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# please change the following according to your system\n",
    "# hyperparameters in this notebook\n",
    "# words in words/words.txt file , one per line , last line has space word\n",
    "# change paths in this notebook to that corresponding to your system\n",
    "\n",
    "\n",
    "# peace.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from python_speech_features import mfcc\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audio_to_mfcc(fileurl):\n",
    "    rate, sig = wav.read(fileurl)\n",
    "    mfcc_feat = mfcc(sig,rate)\n",
    "#d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "#fbank_feat = logfbank(sig,rate)\n",
    "\n",
    "    return mfcc_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = audio_to_mfcc('/home/saurabh/Documents/tf_orange/tf_orange/data/test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_csv(line):\n",
    "       parsed_line = tf.decode_csv(line, [[\"\"],[\"\"]])\n",
    "       \n",
    "    \n",
    "       \n",
    "       \n",
    "\n",
    "       return parsed_line[0] , parsed_line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we also need a fixed vocabulary \n",
    "import re\n",
    "\n",
    "word_dictionary = {}\n",
    "\n",
    "with open(\"/home/saurabh/Documents/tf_orange/tf_orange/words/words.txt\") as file:\n",
    "    for i , line in enumerate(file):\n",
    "        \n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        word_dictionary[line] = i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(word_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def word_to_index(sentence):\n",
    "   \n",
    "    words = sentence.split(' ')\n",
    "    index_list=[]\n",
    "    for word in words:\n",
    "       \n",
    "        if word in word_dictionary:\n",
    "           # print(word)\n",
    "            index_list.insert(len(index_list) , word_dictionary[word])\n",
    "            index_list.insert(len(index_list) , word_dictionary[' '])\n",
    "    index_list.pop()        \n",
    "    return index_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 1, 7, 2, 7, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index('this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _read_py_function(audio, label):\n",
    "    audio =audio_to_mfcc(audio)\n",
    "   \n",
    "    return audio ,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.contrib.data.TextLineDataset(\"/home/saurabh/Documents/tf_orange/tf_orange/data/data.csv\")\n",
    "dataset=dataset.map(decode_csv)\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda audio, label: tuple(tf.py_func(\n",
    "        _read_py_function, [audio, label], [tf.double, label.dtype])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset=dataset.batch(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "item = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is required for CTC Loss\n",
    "# for it's input , first convert transcrition / ground truth to number representation \n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 100\n",
    "#batch_size = 2\n",
    "display_step = 200\n",
    "num_features = 13\n",
    "logs_path = '/home/saurabh/Documents/tf_orange/tf_orange/logs'\n",
    "model_path = '/home/saurabh/Documents/tf_orange/tf_orange/models/model.ckpt'\n",
    "\n",
    "# Network Parameters\n",
    "\n",
    "timesteps = 300 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 9 # words.txt , all words plus space word  ( 10001) + CTC symbol (1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction = tf.nn.softmax(logits)\n",
    "inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "targets = tf.sparse_placeholder(tf.int32)\n",
    "seq_len = tf.placeholder(tf.int32, [None])\n",
    "batch_size = 2\n",
    "\n",
    "#batch_size = tf.placeholder ( tf.int32 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([ 2 * num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell_fw = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    \n",
    "    lstm_cell_bw = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    \n",
    "    # Get lstm cell output\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_cell_fw, lstm_cell_bw, x,\n",
    "    dtype=tf.float32)\n",
    "    \n",
    "    #convert output shape (timesteps * batch * classes ) to (batch*timesteps*classes)\n",
    "    outputs=tf.transpose( outputs , [1, 0, 2])\n",
    "    \n",
    "    outputs=tf.reshape(outputs, [-1,256])\n",
    "    \n",
    "  \n",
    "    res =  tf.matmul(outputs, weights['out']) + biases['out']\n",
    "    \n",
    "    res = tf.reshape(res, [2,300,9])\n",
    "    \n",
    "    return res\n",
    "   # return tf.nn.softmax(tf.matmul(outputs, weights['out']) + biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "logits = RNN(inputs, weights, biases)\n",
    "\n",
    "loss =  tf.nn.ctc_loss ( targets, logits , seq_len , time_major = False)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                           0.9).minimize(cost)\n",
    "\n",
    "\n",
    "# Option 2: tf.contrib.ctc.ctc_beam_search_decoder\n",
    "    # (it's slower but you'll get better results)\n",
    "decoder_input = tf.transpose(logits, [1, 0, 2])\n",
    "\n",
    "decoded, log_prob = tf.nn.ctc_greedy_decoder(decoder_input, seq_len)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need this for decoding word give its index key value \n",
    "\n",
    "def keys_of_value(dct, value):\n",
    "    for k in dct:\n",
    "        if isinstance(dct[k], list):\n",
    "            if value in dct[k]:\n",
    "                return k\n",
    "        else:\n",
    "            if value == dct[k]:\n",
    "                return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# testing above function \n",
    "\n",
    "print(keys_of_value(word_dictionary, 2419))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247.39\n",
      "world helloworldhellohelloworldhelloworldhelloworldhelloworld\n",
      "worldhelloworldhello worldhello worldhello world\n",
      "13459.3\n",
      "\n",
      "\n",
      "98.4617\n",
      " \n",
      "    \n",
      "284.622\n",
      "\n",
      "\n",
      "91.2298\n",
      " \n",
      "   \n",
      "312.493\n",
      "\n",
      "\n",
      "88.8028\n",
      "  \n",
      "  \n",
      "318.379\n",
      "\n",
      "\n",
      "80.4241\n",
      "hello \n",
      "   \n",
      "296.012\n",
      "\n",
      "\n",
      "68.4996\n",
      "  \n",
      "  \n",
      "292.121\n",
      "\n",
      "\n",
      "57.2499\n",
      "  \n",
      "    \n",
      "283.186\n",
      "\n",
      "\n",
      "45.8395\n",
      "hello  hello \n",
      "    \n",
      "270.839\n",
      "\n",
      "\n",
      "35.7694\n",
      "hello hello  \n",
      "    \n",
      "255.429\n",
      "\n",
      "\n",
      "27.4462\n",
      "hello hello  \n",
      "  world \n",
      "235.852\n",
      "\n",
      "\n",
      "21.0659\n",
      "hello hello hello \n",
      "world world \n",
      "212.849\n",
      "\n",
      "\n",
      "14.3629\n",
      "ahello hello hello \n",
      "world worlda \n",
      "185.148\n",
      "\n",
      "\n",
      "19.2512\n",
      "aahello hello hello a\n",
      "world world worldaaaa\n",
      "160.218\n",
      "aa\n",
      "\n",
      "26.6679\n",
      "ahello hello hello \n",
      "world world worldaa\n",
      "139.488\n",
      "\n",
      "\n",
      "8.36696\n",
      "hello hello hello \n",
      "world world world\n",
      "120.948\n",
      "\n",
      "\n",
      "5.76114\n",
      "hello world hello \n",
      "world world world\n",
      "102.179\n",
      "\n",
      "\n",
      "5.13545\n",
      "hello world hello \n",
      "world world world\n",
      "81.4317\n",
      "\n",
      " \n",
      "6.48722\n",
      "hello world hello   \n",
      "world world worldworld  \n",
      "74.2657\n",
      "          \n",
      "         \n",
      "14.0938\n",
      "hello world hello    \n",
      "world world worldworld   \n",
      "76.3079\n",
      "      \n",
      "     \n",
      "8.93419\n",
      "hello world hello \n",
      "world world world\n",
      "64.8335\n",
      "\n",
      " \n",
      "4.13213\n",
      "hello worldhello \n",
      "world world world\n",
      "67.135\n",
      "\n",
      "\n",
      "3.3673\n",
      "hello worldhello \n",
      "world world world\n",
      "65.688\n",
      "\n",
      "\n",
      "3.07936\n",
      "hello worldhello \n",
      "world world world\n",
      "58.0962\n",
      "\n",
      "\n",
      "2.89034\n",
      "hello world hello \n",
      "world world world\n",
      "47.9135\n",
      "\n",
      " \n",
      "3.74934\n",
      "hello world hello  \n",
      "world world world  \n",
      "42.1481\n",
      "    \n",
      "   \n",
      "5.31658\n",
      "hello world hello  \n",
      "world world world  \n",
      "38.7149\n",
      "a\n",
      "   \n",
      "4.52448\n",
      "hello world hello \n",
      "world world world\n",
      "32.7962\n",
      "\n",
      " \n",
      "2.77526\n",
      "hello world hello \n",
      "world world world\n",
      "29.8088\n",
      "\n",
      " \n",
      "1.98876\n",
      "hello world hello \n",
      "world world world\n",
      "27.6771\n",
      "\n",
      " \n",
      "1.84609\n",
      "hello world hello \n",
      "world world world\n",
      "25.1223\n",
      "this\n",
      " \n",
      "2.13717\n",
      "hello world hello \n",
      "world world world\n",
      "23.1584\n",
      "this\n",
      "this thisthis\n",
      "2.64249\n",
      "hello world hello world\n",
      "world world world\n",
      "20.5586\n",
      "\n",
      " this\n",
      "2.96454\n",
      "hello world hello world\n",
      "world world world\n",
      "17.6903\n",
      "\n",
      " \n",
      "3.03023\n",
      "hello world hello world\n",
      "world world world\n",
      "16.171\n",
      "\n",
      " \n",
      "3.00876\n",
      "hello world hello world\n",
      "world world world\n",
      "15.4777\n",
      "\n",
      " \n",
      "3.01703\n",
      "hello world hello world\n",
      "world world world\n",
      "14.951\n",
      "\n",
      " \n",
      "3.08173\n",
      "hello world hello world\n",
      "world world world\n",
      "14.5259\n",
      "\n",
      " \n",
      "3.14376\n",
      "hello world hello world\n",
      "world world world\n",
      "14.2497\n",
      "\n",
      " \n",
      "3.06997\n",
      "hello world hello world\n",
      "world world world\n",
      "14.0357\n",
      "\n",
      " \n",
      "2.81027\n",
      "hello world hello world\n",
      "world world world\n",
      "13.778\n",
      "\n",
      " \n",
      "2.49581\n",
      "hello world hello world\n",
      "world world world\n",
      "13.3966\n",
      "\n",
      " \n",
      "2.2562\n",
      "hello world hello world\n",
      "world world world\n",
      "12.8693\n",
      "\n",
      " \n",
      "2.10621\n",
      "hello world hello world\n",
      "world world world\n",
      "12.324\n",
      "\n",
      " \n",
      "2.00847\n",
      "hello world hello world\n",
      "world world world\n",
      "11.9396\n",
      "\n",
      " \n",
      "1.89327\n",
      "hello world hello world\n",
      "world world world\n",
      "11.7486\n",
      "\n",
      " \n",
      "1.70982\n",
      "hello world hello world\n",
      "world world world\n",
      "11.6361\n",
      "\n",
      " \n",
      "1.4907\n",
      "hello world hello world\n",
      "world world world\n",
      "11.4623\n",
      "\n",
      " \n",
      "1.30576\n",
      "hello world hello world\n",
      "world world world\n",
      "11.2004\n",
      "\n",
      " \n",
      "1.19532\n",
      "hello world hello world\n",
      "world world world\n",
      "10.9385\n",
      "\n",
      " \n",
      "1.14897\n",
      "hello world hello world\n",
      "world world world\n",
      "10.5614\n",
      "a\n",
      " \n",
      "1.09944\n",
      "hello world hello world\n",
      "world world world\n",
      "10.2031\n",
      "a\n",
      " \n",
      "0.971415\n",
      "hello world hello world\n",
      "world world world\n",
      "10.0292\n",
      "\n",
      " \n",
      "0.789945\n",
      "hello world hello world\n",
      "world world world\n",
      "9.9425\n",
      "test\n",
      " \n",
      "0.644477\n",
      "hello world hello world\n",
      "world world world\n",
      "9.93052\n",
      "test\n",
      " \n",
      "0.571508\n",
      "hello world hello world\n",
      "world world world\n",
      "9.87896\n",
      "test\n",
      " \n",
      "0.55123\n",
      "hello world hello world\n",
      "world world world\n",
      "9.74817\n",
      "test\n",
      " \n",
      "0.548277\n",
      "hello world hello world\n",
      "world world world\n",
      "9.63358\n",
      "test\n",
      " \n",
      "0.530208\n",
      "hello world hello world\n",
      "world world world\n",
      "9.54093\n",
      "test\n",
      " \n",
      "0.502285\n",
      "hello world hello world\n",
      "world world world\n",
      "9.47277\n",
      " test\n",
      " \n",
      "0.484291\n",
      "hello world hello world\n",
      "world world world\n",
      "9.37252\n",
      " test\n",
      " \n",
      "0.474781\n",
      "hello world hello world\n",
      "world world world\n",
      "9.24962\n",
      " test\n",
      " \n",
      "0.460656\n",
      "hello world hello world\n",
      "world world world\n",
      "9.15216\n",
      " test\n",
      " \n",
      "0.436809\n",
      "hello world hello world\n",
      "world world world\n",
      "9.07532\n",
      "test\n",
      " \n",
      "0.412827\n",
      "hello world hello world\n",
      "world world world\n",
      "8.9992\n",
      "test\n",
      " \n",
      "0.398385\n",
      "hello world hello world\n",
      "world world world\n",
      "8.9115\n",
      "test\n",
      " \n",
      "0.392686\n",
      "hello world hello world\n",
      "world world world\n",
      "8.82204\n",
      "test\n",
      " \n",
      "0.388766\n",
      "hello world hello world\n",
      "world world world\n",
      "8.72856\n",
      "test\n",
      " \n",
      "0.385499\n",
      "hello world hello world\n",
      "world world world\n",
      "8.6293\n",
      "test\n",
      " \n",
      "0.387127\n",
      "hello world hello world\n",
      "world world world\n",
      "8.51697\n",
      "a test\n",
      " \n",
      "0.393717\n",
      "hello world hello world\n",
      "world world world\n",
      "8.39326\n",
      "is a test\n",
      " \n",
      "0.400126\n",
      "hello world hello world\n",
      "world world world\n",
      "8.2654\n",
      "is a test\n",
      " \n",
      "0.403066\n",
      "hello world hello world\n",
      "world world world\n",
      "8.14266\n",
      "is a test\n",
      " \n",
      "0.404667\n",
      "hello world hello world\n",
      "world world world\n",
      "8.02431\n",
      "is a test\n",
      " \n",
      "0.407001\n",
      "hello world hello world\n",
      "world world world\n",
      "7.91148\n",
      "is a test\n",
      " \n",
      "0.407893\n",
      "hello world hello world\n",
      "world world world\n",
      "7.81336\n",
      "is a test\n",
      " \n",
      "0.404524\n",
      "hello world hello world\n",
      "world world world\n",
      "7.72729\n",
      "is a test\n",
      " \n",
      "0.398309\n",
      "hello world hello world\n",
      "world world world\n",
      "7.64107\n",
      "is a test\n",
      " another \n",
      "0.392231\n",
      "hello world hello world\n",
      "world world world\n",
      "7.54394\n",
      " is a test\n",
      " another test\n",
      "0.386407\n",
      "hello world hello world\n",
      "world world world\n",
      "7.43736\n",
      " is a test\n",
      " another test\n",
      "0.379536\n",
      "hello world hello world\n",
      "world world world\n",
      "7.3294\n",
      " is a test\n",
      " another test\n",
      "0.372337\n",
      "hello world hello world\n",
      "world world world\n",
      "7.23059\n",
      " is   test\n",
      " another test\n",
      "0.366706\n",
      "hello world hello world\n",
      "world world world\n",
      "7.14487\n",
      " is   test\n",
      " another test\n",
      "0.363067\n",
      "hello world hello world\n",
      "world world world\n",
      "7.07014\n",
      " is   test\n",
      " another test\n",
      "0.36053\n",
      "hello world hello world\n",
      "world world world\n",
      "7.00088\n",
      " is   test\n",
      " another test\n",
      "0.358845\n",
      "hello world hello world\n",
      "world world world\n",
      "6.93204\n",
      " is   test\n",
      " another test\n",
      "0.358684\n",
      "hello world hello world\n",
      "world world world\n",
      "6.86099\n",
      " is   test\n",
      " another test\n",
      "0.360159\n",
      "hello world hello world\n",
      "world world world\n",
      "6.78815\n",
      " is a test\n",
      " another test\n",
      "0.362254\n",
      "hello world hello world\n",
      "world world world\n",
      "6.71549\n",
      " is a test\n",
      " another test\n",
      "0.364015\n",
      "hello world hello world\n",
      "world world world\n",
      "6.64401\n",
      "this is a test\n",
      " another test\n",
      "0.365449\n",
      "hello world hello world\n",
      "world world world\n",
      "6.57354\n",
      "this is a test\n",
      " another test\n",
      "0.366729\n",
      "hello world hello world\n",
      "world world world\n",
      "6.50414\n",
      " is a test\n",
      " another test\n",
      "0.367422\n",
      "hello world hello world\n",
      "world world world\n",
      "6.43642\n",
      " is a test\n",
      " another test\n",
      "0.367196\n",
      "hello world hello world\n",
      "world world world\n",
      "6.37021\n",
      "this is a test\n",
      " another test\n",
      "0.366487\n",
      "hello world hello world\n",
      "world world world\n",
      "6.30474\n",
      "this is a test\n",
      " another test\n",
      "0.365828\n",
      "hello world hello world\n",
      "world world world\n",
      "6.23988\n",
      "this is a test\n",
      " another test\n",
      "0.365179\n",
      "hello world hello world\n",
      "world world world\n",
      "6.17595\n",
      "this is a test\n",
      " another test\n",
      "0.364429\n",
      "hello world hello world\n",
      "world world world\n",
      "6.11242\n",
      "this is a test\n",
      " another test\n",
      "0.363851\n",
      "hello world hello world\n",
      "world world world\n",
      "6.04794\n",
      "this is a test\n",
      " another test\n",
      "0.363638\n",
      "hello world hello world\n",
      "world world world\n",
      "5.98129\n",
      "this is a test\n",
      " another test\n",
      "0.363576\n",
      "hello world hello world\n",
      "world world world\n",
      "5.91142\n",
      "this is a test\n",
      " another test\n",
      "11.8228445053\n",
      "Model saved in file: /home/saurabh/Documents/tf_orange/tf_orange/models/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #print(train_seq_len)\n",
    "    \n",
    "    for curr_epoch in range(training_steps):\n",
    "        \n",
    "        sess.run(iterator.initializer)\n",
    "        train_cost = train_ler = 0\n",
    "        start = time.time()\n",
    "        \n",
    "       \n",
    "\n",
    "        while True: \n",
    "            try:\n",
    "                \n",
    "                max_len=timesteps\n",
    "                \n",
    "                \n",
    "              \n",
    "            \n",
    "           \n",
    "                elem = sess.run(item)\n",
    "            #    print(elem[0].shape)\n",
    "            #    print(type(elem[0]))\n",
    "                \n",
    "               # for label in elem[1]:\n",
    "               #     print (word_to_index(label.decode(\"utf-8\")) )\n",
    "                \n",
    "                batch_inputs=elem[0]\n",
    "                \n",
    "                batch_lengths=[]\n",
    "                \n",
    "            \n",
    "                for index,test_item in enumerate(batch_inputs):\n",
    "                \n",
    "                   # print (test_item.shape)\n",
    "                   # max_len=max(max_len, test_item.shape[0])\n",
    "                    batch_lengths.insert(index, test_item.shape[0])\n",
    "                    \n",
    "                    \n",
    "                temp_np_inputs = np.zeros((batch_size , max_len , 13 ))\n",
    "                    \n",
    "                for index,test_item in enumerate(batch_inputs):\n",
    "                \n",
    "                    if test_item.shape[0] < max_len:\n",
    "              #          print(\"needs padding\")\n",
    "                        difference = max_len - test_item.shape[0]\n",
    "              #          print(\"difference : \" + str(difference))\n",
    "                        temp_np_inputs[index] = np.pad(test_item,((0,difference),(0,0)), mode=\"constant\")\n",
    "        \n",
    "             #   print('max_len : ' + str(max_len) )\n",
    "            \n",
    "                batch_list = []\n",
    "                \n",
    "             #   print(batch_lengths)\n",
    "                \n",
    "                \n",
    "            #    print(temp_np_inputs[0].shape)\n",
    "                \n",
    "            \n",
    "                for index,j in enumerate(elem[1]):\n",
    "                    batch_list.insert(index, word_to_index(j.decode(\"utf-8\")))\n",
    "                    \n",
    "                \n",
    "                batch_targets = sparse_tuple_from(batch_list)\n",
    "                \n",
    "                \n",
    "                \n",
    "             \n",
    "                \n",
    "                \n",
    "            \n",
    "                feed = {inputs: temp_np_inputs,\n",
    "                        targets: batch_targets,\n",
    "                        seq_len: batch_lengths,\n",
    "                         }\n",
    "\t    \n",
    "\n",
    "                batch_cost, _ , summary = sess.run([cost, optimizer , merged_summary_op ], feed)\n",
    "                print(batch_cost)\n",
    "                \n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, curr_epoch )\n",
    "                \n",
    "                # Decoding\n",
    "                d = sess.run(decoded[0], feed_dict=feed)\n",
    "   \n",
    "                dense_decoded = tf.sparse_tensor_to_dense(d, default_value=-1).eval(session=sess)\n",
    "    \n",
    "                for i, seq in enumerate(dense_decoded):\n",
    "\n",
    "                    seq = [s for s in seq if s != -1]\n",
    "       \n",
    "                    str_decoded = ''.join([keys_of_value(word_dictionary, x) for x in seq ])\n",
    "                    print(str_decoded)\n",
    "                \n",
    "            except tf.errors.OutOfRangeError:\n",
    "            #    print(\"End of training dataset.\")\n",
    "                break\n",
    "           \n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    train_cost += batch_cost*batch_size\n",
    "    print(train_cost)\n",
    "    \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_orange",
   "language": "python",
   "name": "tf_orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
